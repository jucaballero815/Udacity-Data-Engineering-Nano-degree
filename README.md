# Introduction
This is a description of a __data engineering project from Udacity's Data__ 
__Engineering nano-degree program__.  It involves data modeling with a PostgreSQL database schema and creating ETL processes that load source data into the PostgreSQL database.  

# Project Purpose
The purpose of the project is to help a fictional startup Sparkify analyze data they've collected on songs and user activity on their new music streaming app.  Sparkify is particularly interested in understanding what songs users listen to, but they don't currently have a convenient way of querying their source data.  This data is stored in two directories:  one contains JSON logs on user activity on the app, and the other contains JSON metadata on the songs in their app.  In this project, several ETL processes are built that load source data into a PostgreSQL database with a schema that is designed to optimize song play analysis.  

# Source data
## Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
Here are the contents of the song_data/A/A/B/TRAABJL12903CDCF1A.json song file:
```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset
The second dataset consists of log files in JSON format generated by [this event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
Displayed below are some of the contents of 2018/11/2018-11-13-events.json after it has been loaded into a 
dataframe:

![sdfds](images/dataframe-image.png)

# Database Schema Design
A star schema is used to optimize queries on song-play analysis.  It consists of the following tables:

## Fact Table
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
    - *songplay_id*, *start_time*, *user_id*, *level*, *song_id*, *artist_id*, *session_id*, *location*, *user_agent*

## Dimension Tables
2. **users** - users in the app
    - *user_id*, *first_name*, *last_name*, *gender*, *level*
3. **songs** - songs in the music database
    - *song_id*, *title*, *artist_id*, *year*, *duration*
4. **artists** - artists in music database
    - *artist_id*, *name*, *location*, *latitude*, *longitude*
5. **time** - timestamps of records in songplays broken down into specific units
    - *start_time*, *hour*, *day*, *week*, *month*, *year*, *weekday*
    
# Source Code Files:
The project workspace contains the following six source code files aside from this README.md:

1. `test.ipynb` displays the first few rows of each table to let you check your database.
2. `create_tables.py` drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
3. `etl.ipynb` reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
4. `etl.py` reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
5. `sql_queries.py` contains all your sql queries, and is imported into the last three files above.

Note that all the source code files are meant to be loaded and run in a __JupyterLab__ environment.    The python files in particular can be run in a terminal launched in a __JupyterLab__ environment.  In the project workspace, select a new terminal launcher. 

![new launcher](images/launcher.png)

Run a python file by typing `python file_name.py` in the terminal.  In the image below, the `create_tables.py` file ran successfully in the terminal.   

![run python file in terminal](images/terminal.png)

# ETL Processes
The `etl.ipynb` notebook contains ETL processes for each of the five tables in our star schema.  They can be run one at a time.  The `test.ipynb` notebook can be run at the end of each ETL process or at the end of the entire `etl.ipynb` notebook to check that the ETL processes ran successfully.  The `create_tables.py` script can be run to reset the tables before running `test.ipynb`.

The `etl.py` script is the central ETL pipeline of this project.  It loads the data into all of the tables at once.  As with `etl.ipynb`, one can run `create_tables.py` before running `etl.py` to reset tables and `test.ipynb` to check if records were successfully inserted into the tables. 

# Solution Dataset
Of particular interest is the data loaded into the **songplays table**, which is the fact table in our star schema.
This table is a subset of the much larger [Million Song Dataset](http://millionsongdataset.com/), and **it only has one row with non-null values for both songid and artistid**. Those are the only two values for these fields that the query in the `sql_queries.py` will return that are non-null. The rest of the rows will have NONE values for both of these fields.
